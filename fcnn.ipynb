{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MiO9dItusBX"
      },
      "source": [
        "# <center>Fully Connected Neural Network</center>\n",
        "\n",
        "In this assignment, you will learn how to build a fully connected neural network using NumPy. Note that you **cannot** use Pytorch for this assignment.\n",
        "\n",
        "## 1 -  Neural network architecture\n",
        "You will build a fully connected neural network with 4 layers: an input layer, two hidden layers, and an output layer. Each layer will be fully connected, and the network will use the sigmoid activation function. Optimization will be done using stochastic gradient descent.\n",
        "\n",
        "Let's define the layers in detail. Since the task is to classify digits, the neural network will output probabilities indicating how likely an input image belongs to each class.\n",
        "\n",
        "1. **Input Layer**: The input consists of 28x28 pixel images. These images will be flattened into a one-dimensional array of size $ 28 \\times 28 = 784 $ elements. Therefore, the input layer will have 784 nodes.\n",
        "   \n",
        "2. **1st Hidden Layer**: This layer will reduce the number of nodes from 784 (in the input layer) to 64. The goal here is to learn a compact representation of the input features.\n",
        "\n",
        "3. **2nd Hidden Layer**: This layer will reduce the number of nodes from 64 (in the 1st Hidden Layer) to 32. The goal here is to learn a compact representation of the input features.\n",
        "   \n",
        "4. **Output Layer**: The 2nd hidden layer’s 32 nodes will be further reduced to 10 nodes in the output layer. This corresponds to the 10 possible digit classes (0 through 9). The output will be an array of 10 elements, where each element represents the probability that the input image belongs to that class. The correct class will have a label of 1, while all others will be labeled 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5cmYgBvusBZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcPnH9GuusBa"
      },
      "source": [
        "## 2 - Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkFmhtHhusBa"
      },
      "outputs": [],
      "source": [
        "# Helper function\n",
        "def show_images(image, num_row=2, num_col=5):\n",
        "    # plot images\n",
        "    image_size = int(np.sqrt(image.shape[-1]))\n",
        "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
        "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "    for i in range(num_row*num_col):\n",
        "        ax = axes[i//num_col, i%num_col]\n",
        "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def one_hot(x, k, dtype=np.float32):\n",
        "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "    return np.array(x[:, None] == np.arange(k), dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBHyeVEwusBa"
      },
      "source": [
        "## 3 - Dataset preparation\n",
        "In this assignment, you will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to demonstrate how to build a neural network. The MNIST dataset contains 70,000 grayscale images of handwritten digits, with 60,000 used for training and 10,000 for testing. Each image is \\(28 x 28\\) pixels, and the pixel values range from 0 to 255.\n",
        "\n",
        "### Note\n",
        "To load the dataset, `fetch_openml` requires `scikit-learn >= 0.22`. If you are using an older version, replace `from sklearn.datasets import fetch_openml` with `from sklearn.datasets import fetch_mldata`.\n",
        "\n",
        "Next, we will load and preprocess the dataset so it can be used in NumPy. The images will be normalized by dividing all pixel values by 255, which scales the data to a range between 0 and 1. This helps avoid numerical stability issues with activation functions. Additionally, we will use one-hot encoding for the labels, which makes it easier to compute the error between the network's output and the true labels. Lastly, we will flatten each image into an array of \\(28 x 28 = 784\\) elements to match the input layer's requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-sVZhlpxusBb"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "mnist_data = fetch_openml(\"mnist_784\")\n",
        "x = mnist_data[\"data\"]\n",
        "y = mnist_data[\"target\"].astype(int)\n",
        "\n",
        "# Normalize\n",
        "x /= 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "num_labels = 10\n",
        "examples = y.shape[0]\n",
        "y_new = one_hot(np.array(y), num_labels)\n",
        "\n",
        "# Split, reshape, shuffle\n",
        "train_size = 60000\n",
        "test_size = x.shape[0] - train_size\n",
        "x_train, x_test = x[:train_size].to_numpy(), x[train_size:].to_numpy()\n",
        "y_train, y_test = y_new[:train_size], y_new[train_size:]\n",
        "\n",
        "shuffle_index = np.random.permutation(train_size)\n",
        "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "_SikO-G2usBb",
        "outputId": "59647e4b-e005-406c-fa5f-6e87444cbfb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: (60000, 784) (60000, 10)\n",
            "Test data: (10000, 784) (10000, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
        "print(\"Test data: {} {}\".format(x_test.shape, y_test.shape))\n",
        "show_images(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmlQiulAusBc"
      },
      "source": [
        "## 4 - Fully Connected Neural Network with Numpy\n",
        "\n",
        "\n",
        "### Initialization\n",
        "The neural network transforms data from 784 input nodes down to 10 output nodes. This transformation is achieved by defining the number of nodes in each layer through an array of sizes passed when initializing the `DeepNeuralNetwork` class. For instance, specifying `[784, 64, 32, 10]` would create a network with an input layer of 784 nodes, a hidden layer of 64 nodes, a hidden layer of 32 nodes, and an output layer of 10 nodes.\n",
        "\n",
        "To optimize the network’s performance, it’s crucial to initialize weights carefully. A common approach is to scale the weights’ variance in each layer to stabilize learning. Following a technique recommended by Andrew Ng, we set the variance for each layer's weights to $\\frac{1}{n} $, where $n$ is the number of nodes feeding into that layer. To achieve this, we initialize weights using values from a standard normal distribution, then scale by $\\frac{1}{\\sqrt{n}}$, where $n$ is the number of inputs to the layer.\n",
        "\n",
        "Weight initialization can be complex, especially because it requires an understanding of linear algebra to grasp how dimensions interact in dot products. This is beyond the current scope, but this approach helps in stabilizing the forward pass and ultimately contributes to more effective training of the neural network.\n",
        "\n",
        "### Feedforward\n",
        "The forward pass in this neural network primarily involves matrix multiplication, which in NumPy is performed with the dot operation. In detail, we multiply the weights by the activations of the previous layer and then apply the sigmoid activation function to the result.\n",
        "\n",
        "To propagate through each layer, we sequentially apply the dot operation, followed by the sigmoid activation function. In the final layer, we use the softmax function to generate probabilities for each class, allowing us to measure how well our current forward pass performs by comparing the predicted probabilities with the true class labels.\n",
        "\n",
        "\n",
        "### Backpropagation\n",
        "\n",
        "In this task, you'll implement backpropagation to update the weights and biases of a neural network based on the gradients of the loss function.\n",
        "\n",
        "1. **Calculate Output Layer Gradients**: Compute the error for the output layer by taking the difference between the network’s predictions and the actual labels. Use this to calculate the weight and bias gradients for the output layer.\n",
        "\n",
        "2. **Backpropagate to Hidden Layers**: Propagate the error from the output layer back to the hidden layer. This involves multiplying the output layer’s gradients by the weights of that layer, then adjusting for the activation function’s derivative.\n",
        "\n",
        "3. **Store and Return Gradients**: Organize the computed gradients for each layer’s weights and biases, so they’re ready for use in the parameter update step.\n",
        "\n",
        "This process should result in a dictionary of gradients for efficient optimization.\n",
        "\n",
        "### Training (Stochastic Gradient Descent)\n",
        "To train our neural network, we need a training loop that combines the forward and backward passes with an optimization step. Here, we’ll use Stochastic Gradient Descent (SGD) as the optimizer to update the network’s parameters.\n",
        "\n",
        "The training function consists of two main loops:\n",
        "\n",
        "1. **Epoch Loop**: This outer loop iterates through the dataset a set number of times (epochs).\n",
        "2. **Batch Loop**: For each epoch, this inner loop processes each batch of data individually.\n",
        "\n",
        "For each batch, the process includes:\n",
        "\n",
        "- **Forward Pass**: Call `self.feed_forward()` with `x`, a batch of input data (flattened into an array of length 784). The function returns `output`, the network's prediction.\n",
        "- **Backward Pass**: Using the `output` from the forward pass and the one-hot encoded ground truth labels `y`, the backward pass (`self.back_propagate()`) calculates the gradients for the weights.\n",
        "- **Optimization with SGD**: The `optimize()` function then updates the parameters based on the gradients using SGD, which adjusts each parameter directly in the direction of the calculated gradient.\n",
        "\n",
        "After each epoch, we can assess the network’s performance by calculating accuracy on a validation set. This provides a check on how well the model generalizes beyond the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrlFukTwusBc"
      },
      "outputs": [],
      "source": [
        "class DeepNeuralNetwork():\n",
        "    def __init__(self, sizes):\n",
        "        self.sizes = sizes\n",
        "\n",
        "        self.activation = self.sigmoid\n",
        "\n",
        "        # Initialize all the weights and biases\n",
        "        self.params = self.initialize()\n",
        "        # Save all intermediate values, i.e. activations\n",
        "        self.cache = {}\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "\n",
        "        if derivative:\n",
        "            ### START CODE HERE ### (≈ 1 line of code)\n",
        "            # Calculate the derivative of the sigmoid function\n",
        "            return x * (1 - x)\n",
        "\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        # Implement the forward pass of the sigmoid function\n",
        "\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def softmax(self, x):\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Calculate softmax loss function\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def initialize(self):\n",
        "        # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_layer_1=self.sizes[1]\n",
        "        hidden_layer_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            \"W1\": np.random.randn(hidden_layer_1, input_layer) * np.sqrt(1./input_layer),\n",
        "            \"b1\": np.zeros((hidden_layer_1, 1)),\n",
        "            \"W2\": np.random.randn(hidden_layer_2, hidden_layer_1) * np.sqrt(1./hidden_layer_1),\n",
        "            \"b2\": np.zeros((hidden_layer_2, 1)),\n",
        "            \"W3\": np.random.randn(output_layer, hidden_layer_2) * np.sqrt(1./hidden_layer_2),\n",
        "            \"b3\": np.zeros((output_layer, 1))\n",
        "        }\n",
        "        return params\n",
        "\n",
        "    def feed_forward(self, x):\n",
        "\n",
        "        self.cache[\"X\"] = x\n",
        "        ### START CODE HERE ###\n",
        "        # Implement forward pass for each layer\n",
        "\n",
        "        self.cache[\"Z1\"] = np.dot(x, self.params[\"W1\"].T) + self.params[\"b1\"].T\n",
        "        self.cache[\"A1\"] = self.sigmoid(self.cache[\"Z1\"])\n",
        "        self.cache[\"Z2\"] = np.dot(self.cache[\"A1\"], self.params[\"W2\"].T) + self.params[\"b2\"].T\n",
        "        self.cache[\"A2\"] = self.sigmoid(self.cache[\"Z2\"])\n",
        "        self.cache[\"Z3\"] = np.dot(self.cache[\"A2\"], self.params[\"W3\"].T) + self.params[\"b3\"].T\n",
        "        self.cache[\"A3\"] = self.softmax(self.cache[\"Z3\"])\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return self.cache[\"A3\"]\n",
        "\n",
        "    def back_propagate(self, y, output):\n",
        "\n",
        "        current_batch_size = y.shape[0]\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Calculate gradients for each layer's weights and biases\n",
        "\n",
        "        dZ3 = output - y\n",
        "        dW3 = np.dot(dZ3.T, self.cache[\"A2\"]) / current_batch_size\n",
        "        db3 = dZ3.sum(axis=0).reshape(-1, 1) / current_batch_size\n",
        "\n",
        "        dA2 = np.dot(dZ3, self.params[\"W3\"])\n",
        "        dZ2 = dA2 * self.sigmoid(self.cache[\"A2\"], derivative=True)\n",
        "        dW2 = np.dot(dZ2.T, self.cache[\"A1\"]) / current_batch_size\n",
        "        db2 = dZ2.sum(axis=0).reshape(-1, 1) / current_batch_size\n",
        "\n",
        "        dA1 = np.dot(dZ2, self.params[\"W2\"])\n",
        "        dZ1 = dA1 * self.sigmoid(self.cache[\"A1\"], derivative=True)\n",
        "        dW1 = np.dot(dZ1.T, self.cache[\"X\"]) / current_batch_size\n",
        "        db1 = dZ1.sum(axis=0).reshape(-1, 1) / current_batch_size\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2, \"W3\": dW3, \"b3\": db3}\n",
        "        return self.grads\n",
        "\n",
        "    def cross_entropy_loss(self, y, output):\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Compute cross-entropy loss\n",
        "        return -np.sum(y * np.log(output)) / y.shape[0]\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def optimize(self, l_rate=0.1):\n",
        "        ### START CODE HERE ### (≈ 3 lines of code)\n",
        "        # Update each parameter using the calculated gradients in self.grads\n",
        "        for param in self.params.keys():\n",
        "            self.params[param] -= l_rate * self.grads[param]\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def accuracy(self, y, output):\n",
        "        ### START CODE HERE ###\n",
        "        # Calculate accuracy by comparing predictions to true labels\n",
        "        predictions = np.argmax(output, axis=1)\n",
        "        labels = np.argmax(y, axis=1)\n",
        "        return np.mean(predictions == labels)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def train(self, x_train, y_train, x_test, y_test, epochs=10,\n",
        "              batch_size=64, l_rate=0.1):\n",
        "        # Hyperparameters\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        num_batches = (x_train.shape[0] // self.batch_size)\n",
        "\n",
        "        start_time = time.time()\n",
        "        template = \"Epoch {}: {:.2f}s, train acc={:.2f}, train loss={:.2f}, test acc={:.2f}, test loss={:.2f}\"\n",
        "\n",
        "        # Train\n",
        "        for i in range(self.epochs):\n",
        "            # Shuffle\n",
        "            permutation = np.random.permutation(x_train.shape[0])\n",
        "            x_train_shuffled = x_train[permutation]\n",
        "            y_train_shuffled = y_train[permutation]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "                # Batch\n",
        "                begin = j * self.batch_size\n",
        "                end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
        "                x = x_train_shuffled[begin:end]\n",
        "                y = y_train_shuffled[begin:end]\n",
        "\n",
        "                ### START CODE HERE ### (≈ 3 lines of code)\n",
        "                # Implement forward pass, backpropagation, and optimization\n",
        "                # Implement forward pass\n",
        "                output = self.feed_forward(x)\n",
        "\n",
        "                # Backpropagation\n",
        "                self.back_propagate(y, output)\n",
        "\n",
        "                # Optimization\n",
        "                self.optimize(l_rate)\n",
        "\n",
        "\n",
        "                ### END CODE HERE ###\n",
        "\n",
        "\n",
        "            # Evaluate performance\n",
        "            # Training data\n",
        "            output = self.feed_forward(x_train)\n",
        "            train_acc = self.accuracy(y_train, output)\n",
        "            train_loss = self.cross_entropy_loss(y_train, output)\n",
        "            # Test data\n",
        "            output = self.feed_forward(x_test)\n",
        "            test_acc = self.accuracy(y_test, output)\n",
        "            test_loss = self.cross_entropy_loss(y_test, output)\n",
        "            print(template.format(i+1, time.time()-start_time, train_acc, train_loss, test_acc, test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucn0HiMmusBd"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aInL8B_KusBd",
        "outputId": "3c1a2812-5e7a-4195-cc35-bf67be17b654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: 3.33s, train acc=0.94, train loss=0.20, test acc=0.94, test loss=0.21\n",
            "Epoch 2: 5.25s, train acc=0.97, train loss=0.11, test acc=0.96, test loss=0.13\n",
            "Epoch 3: 7.03s, train acc=0.98, train loss=0.08, test acc=0.97, test loss=0.11\n",
            "Epoch 4: 8.85s, train acc=0.98, train loss=0.07, test acc=0.97, test loss=0.10\n",
            "Epoch 5: 10.88s, train acc=0.98, train loss=0.06, test acc=0.97, test loss=0.10\n",
            "Epoch 6: 14.46s, train acc=0.98, train loss=0.05, test acc=0.97, test loss=0.09\n",
            "Epoch 7: 18.12s, train acc=0.98, train loss=0.06, test acc=0.97, test loss=0.10\n",
            "Epoch 8: 21.85s, train acc=0.99, train loss=0.04, test acc=0.97, test loss=0.09\n",
            "Epoch 9: 25.26s, train acc=0.99, train loss=0.03, test acc=0.98, test loss=0.08\n",
            "Epoch 10: 28.89s, train acc=0.99, train loss=0.03, test acc=0.97, test loss=0.08\n"
          ]
        }
      ],
      "source": [
        "dnn = DeepNeuralNetwork(sizes=[784, 64, 32, 10])\n",
        "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, l_rate=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbJR4I0e4Rf0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}